---
- name: Validate required variables
  ansible.builtin.assert:
    that:
      - model_namespace is defined
      - model_namespace | length > 0
      - model_storage_uri is defined
      - model_storage_uri | length > 0
    fail_msg: "Required variables missing. You must set 'model_namespace' and 'model_storage_uri' in your inventory."
    success_msg: "Required variables validated"

- name: Create namespace for model deployment
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig_path }}"
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ model_namespace }}"

- name: Build vLLM base arguments list
  ansible.builtin.set_fact:
    vllm_args:
      - "--port=8080"
      - "--model=/mnt/models"
      - "--served-model-name={{ '{{.Name}}' }}"
      - "--max-model-len={{ vllm_max_model_len }}"
      - "--gpu-memory-utilization={{ vllm_gpu_memory_utilization }}"

- name: Add chunked prefill flag if enabled
  ansible.builtin.set_fact:
    vllm_args: "{{ vllm_args + ['--enable-chunked-prefill'] }}"
  when: vllm_enable_chunked_prefill | bool

- name: Add chunked prefill flag if disabled
  ansible.builtin.set_fact:
    vllm_args: "{{ vllm_args + ['--no-enable-chunked-prefill'] }}"
  when: not (vllm_enable_chunked_prefill | bool)

- name: Add additional vLLM arguments
  ansible.builtin.set_fact:
    vllm_args: "{{ vllm_args + vllm_additional_args }}"
  when: vllm_additional_args | length > 0

- name: Create ServingRuntime for vLLM
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig_path }}"
    state: present
    definition:
      apiVersion: serving.kserve.io/v1alpha1
      kind: ServingRuntime
      metadata:
        name: "{{ runtime_name }}"
        namespace: "{{ model_namespace }}"
        labels:
          opendatahub.io/dashboard: "true"
        annotations:
          opendatahub.io/accelerator-name: migrated-gpu
          opendatahub.io/apiProtocol: REST
          opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
          opendatahub.io/template-display-name: vLLM NVIDIA GPU ServingRuntime for KServe
          opendatahub.io/template-name: vllm-cuda-runtime
          openshift.io/display-name: "{{ model_display_name }}"
      spec:
        multiModel: false
        supportedModelFormats:
          - name: "{{ model_format }}"
            autoSelect: true
        containers:
          - name: kserve-container
            image: "{{ runtime_image }}"
            command:
              - python
              - -m
              - vllm.entrypoints.openai.api_server
            args: "{{ vllm_args }}"
            env:
              - name: HF_HOME
                value: /tmp/hf_home
            ports:
              - containerPort: 8080
                protocol: TCP
            volumeMounts:
              - name: shm
                mountPath: /dev/shm
        volumes:
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: "{{ shm_size }}"
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"

- name: Build InferenceService annotations
  ansible.builtin.set_fact:
    isvc_annotations:
      openshift.io/display-name: "{{ model_display_name }}"
      serving.kserve.io/deploymentMode: RawDeployment

- name: Add auth annotation if enabled
  ansible.builtin.set_fact:
    isvc_annotations: "{{ isvc_annotations | combine({'security.opendatahub.io/enable-auth': 'true'}) }}"
  when: enable_auth | bool

- name: Create InferenceService for model
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig_path }}"
    state: present
    definition:
      apiVersion: serving.kserve.io/v1beta1
      kind: InferenceService
      metadata:
        name: "{{ model_name }}"
        namespace: "{{ model_namespace }}"
        labels:
          opendatahub.io/dashboard: "true"
        annotations: "{{ isvc_annotations }}"
      spec:
        predictor:
          automountServiceAccountToken: false
          minReplicas: "{{ model_replicas_min }}"
          maxReplicas: "{{ model_replicas_max }}"
          tolerations: "{{ gpu_tolerations }}"
          model:
            modelFormat:
              name: "{{ model_format }}"
            name: ""
            runtime: "{{ runtime_name }}"
            storageUri: "{{ model_storage_uri }}"
            resources:
              requests:
                cpu: "{{ model_cpu_request }}"
                memory: "{{ model_memory_request }}"
                nvidia.com/gpu: "{{ model_gpu_request }}"
              limits:
                cpu: "{{ model_cpu_limit }}"
                memory: "{{ model_memory_limit }}"
                nvidia.com/gpu: "{{ model_gpu_limit }}"

- name: Create ServiceAccount for model authentication
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig_path }}"
    state: present
    definition:
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: "{{ model_name }}-sa"
        namespace: "{{ model_namespace }}"
  when: enable_auth | bool

- name: Create Role to allow getting the InferenceService
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig_path }}"
    state: present
    definition:
      apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      metadata:
        name: "{{ model_name }}-auth-role"
        namespace: "{{ model_namespace }}"
      rules:
      - apiGroups: ["serving.kserve.io"]
        resources: ["inferenceservices"]
        resourceNames: ["{{ model_name }}"]
        verbs: ["get"]
  when: enable_auth | bool

- name: Create RoleBinding for ServiceAccount
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig_path }}"
    state: present
    definition:
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: "{{ model_name }}-auth-rolebinding"
        namespace: "{{ model_namespace }}"
      subjects:
      - kind: ServiceAccount
        name: "{{ model_name }}-sa"
        namespace: "{{ model_namespace }}"
      roleRef:
        kind: Role
        name: "{{ model_name }}-auth-role"
        apiGroup: rbac.authorization.k8s.io
  when: enable_auth | bool

- name: Create token Secret for ServiceAccount
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig_path }}"
    state: present
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: "{{ model_name }}-sa-token"
        namespace: "{{ model_namespace }}"
        labels:
          opendatahub.io/dashboard: "true"
        annotations:
          kubernetes.io/service-account.name: "{{ model_name }}-sa"
          openshift.io/display-name: "{{ model_name }}"
      type: kubernetes.io/service-account-token
  when: enable_auth | bool

- name: Wait for InferenceService to be ready
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig_path }}"
    api_version: serving.kserve.io/v1beta1
    kind: InferenceService
    name: "{{ model_name }}"
    namespace: "{{ model_namespace }}"
  register: inference_service
  until:
    - inference_service.resources | length > 0
    - inference_service.resources[0].status is defined
    - inference_service.resources[0].status.conditions is defined
    - inference_service.resources[0].status.conditions | selectattr('type', 'equalto', 'Ready') | selectattr('status', 'equalto', 'True') | list | length > 0
  retries: "{{ wait_retries }}"
  delay: "{{ wait_delay }}"
  when: wait_for_ready | bool

- name: Get InferenceService URL
  ansible.builtin.set_fact:
    model_url: "{{ inference_service.resources[0].status.url }}"
  when:
    - wait_for_ready | bool
    - inference_service.resources | length > 0
    - inference_service.resources[0].status.url is defined

- name: Get authentication token
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig_path }}"
    api_version: v1
    kind: Secret
    name: "{{ model_name }}-sa-token"
    namespace: "{{ model_namespace }}"
  register: auth_token_secret
  when: enable_auth | bool

- name: Extract token value
  ansible.builtin.set_fact:
    model_auth_token: "{{ auth_token_secret.resources[0].data.token | b64decode }}"
  when:
    - enable_auth | bool
    - auth_token_secret.resources | length > 0

- name: Display model deployment summary
  ansible.builtin.debug:
    msg:
      - "Model deployment complete!"
      - "Model: {{ model_name }}"
      - "Namespace: {{ model_namespace }}"
      - "Runtime: {{ model_format }} ({{ runtime_name }})"
      - "URL: {{ model_url | default('N/A') }}"
      - "Max Context Length: {{ vllm_max_model_len }}"
      - "GPU Memory Utilization: {{ vllm_gpu_memory_utilization }}"
      - "Authentication: {{ 'Enabled' if enable_auth else 'Disabled' }}"
      - "{% if enable_auth %}Token Secret: {{ model_name }}-sa-token{% endif %}"
      - "{% if enable_auth and model_auth_token is defined %}Auth Token: {{ model_auth_token[:50] }}...{% endif %}"
