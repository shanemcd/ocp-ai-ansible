---
# KServe Model Deployment Configuration

# Model configuration
model_name: my-model
model_display_name: "{{ model_name }}"
model_namespace: ""  # REQUIRED: Namespace for model deployment
model_storage_uri: ""  # REQUIRED: OCI or S3 URI for the model
model_format: vLLM  # vLLM, pytorch, tensorflow, etc.

# Runtime configuration
runtime_name: "{{ model_name }}"
runtime_image: registry.redhat.io/rhoai/odh-vllm-cuda-rhel9@sha256:fb84fbf103bf450ef5b060fc5f21a9cf16b166dba207a3c50aa91bccd919d604

# vLLM specific configuration
vllm_max_model_len: 32768  # Maximum context length (reduce if OOM)
vllm_gpu_memory_utilization: 0.9  # Fraction of GPU memory to use
vllm_enable_chunked_prefill: false  # Disable for compatibility
vllm_additional_args: []  # Additional vLLM arguments

# Resource configuration
model_replicas_min: 1
model_replicas_max: 1
model_cpu_request: "1"
model_cpu_limit: "2"
model_memory_request: 4Gi
model_memory_limit: 8Gi
model_gpu_request: 1
model_gpu_limit: 1

# Shared memory size for vLLM
shm_size: 2Gi

# Authentication
enable_auth: true
auth_token_name: "{{ model_name }}"  # Name for the auth token secret

# Tolerations for GPU nodes
gpu_tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Wait for readiness
wait_for_ready: true
wait_retries: 60
wait_delay: 10
